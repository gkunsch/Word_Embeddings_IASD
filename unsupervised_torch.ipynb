{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python unsupervised.py --src_lang en --tgt_lang fr --src_emb ../wiki.en.vec --tgt_emb ../wiki.fr.vec --n_refinement 5 --epoch_size 100000 --max_vocab 30000 --dis_most_frequent 30000 --dico_eval ../data/en-fr.txt --batch_size 32 --dico_method nn --dico_max_size 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python GAN.py --src_emb wiki.en.vec --tgt_emb wiki.fr.vec --batch_size 32 --epoch_size 50000 --n_epochs 20 --dico_eval data/en-fr.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "from numba import jit,cuda\n",
    "\n",
    "def load_vec(emb_path, nmax=50000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = 'wiki.en.vec'\n",
    "tgt_path = 'wiki.fr.vec'\n",
    "\n",
    "nmax = 100000  # maximum number of word embeddings to load\n",
    "\n",
    "src_embeddings, src_id2word, src_word2id = load_vec(src_path, nmax)\n",
    "tgt_embeddings, tgt_id2word, tgt_word2id = load_vec(tgt_path, nmax)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_with_emb(word_emb ,tgt_emb, tgt_id2word, K=5):\n",
    "\n",
    "    scores = (tgt_emb / np.linalg.norm(tgt_emb, 2, 1)[:, None]).dot(word_emb / np.linalg.norm(word_emb))\n",
    "    k_best = scores.argsort()[-K:][::-1]\n",
    "    #for i, idx in enumerate(k_best):\n",
    "    #    print('%.4f - %s' % (scores[idx], tgt_id2word[idx]))\n",
    "    return scores[k_best[0]],tgt_id2word[k_best[0]],scores,k_best  #Return le mot le plus proche de l'embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"data/en-fr.txt\", \"r\",encoding=\"utf-8\")\n",
    "fr_list=[]\n",
    "eng_list=[]\n",
    "eng_fr_dict={}\n",
    "fr_eng_dict={}\n",
    "for line in f.readlines():\n",
    "\n",
    "    eng_word=line.split()[0]\n",
    "    eng_list.append(eng_word)\n",
    "\n",
    "    fr_word=line.split()[1]\n",
    "    fr_list.append(fr_word)\n",
    "\n",
    "    if eng_word not in eng_fr_dict.keys():\n",
    "        eng_fr_dict[eng_word]=[fr_word]\n",
    "    else:\n",
    "        eng_fr_dict[eng_word].append(fr_word)\n",
    "\n",
    "    if fr_word not in fr_eng_dict.keys():\n",
    "        fr_eng_dict[fr_word]=[eng_word]\n",
    "    else:\n",
    "        fr_eng_dict[fr_word].append(eng_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(eng_fr_dict,fr_eng_dict,src_word2id,tgt_word2id):\n",
    "    \n",
    "    \n",
    "    for word in list(eng_fr_dict.keys()):\n",
    "        if word not in src_word2id.keys():\n",
    "            del eng_fr_dict[word]\n",
    "        else:\n",
    "            if eng_fr_dict[word][0] not in tgt_word2id.keys():\n",
    "                del eng_fr_dict[word]\n",
    "    \n",
    "\n",
    "    for word in list(fr_eng_dict.keys()):\n",
    "        if word not in tgt_word2id.keys():\n",
    "            del fr_eng_dict[word]\n",
    "        else:\n",
    "            if fr_eng_dict[word][0] not in src_word2id.keys():\n",
    "                del fr_eng_dict[word]\n",
    "\n",
    "    #Separate train/test\n",
    "\n",
    "\n",
    "    return eng_fr_dict,fr_eng_dict\n",
    "\n",
    "eng_fr_dict,fr_eng_dict=get_train_test(eng_fr_dict,fr_eng_dict,src_word2id,tgt_word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_embeddings_normalized=src_embeddings.copy()\n",
    "tgt_embeddings_normalized=tgt_embeddings.copy()\n",
    "\n",
    "\n",
    "for i in range(len(src_embeddings_normalized)):\n",
    "    src_embeddings_normalized[i]=src_embeddings_normalized[i]/np.linalg.norm(src_embeddings_normalized[i])\n",
    "    tgt_embeddings_normalized[i]=tgt_embeddings_normalized[i]/np.linalg.norm(tgt_embeddings_normalized[i])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "W=torch.load(\"dumps/en_fr_mapping.pth\")\n",
    "print(type(W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5565839694724418,\n",
       " 'chat',\n",
       " array([0.08456691, 0.26493092, 0.11533649, ..., 0.22230542, 0.14808035,\n",
       "        0.15922943]),\n",
       " array([ 3359,  6351, 26555,  2624,  8470], dtype=int64))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(src_id2word[2570])\n",
    "\n",
    "get_nn_with_emb(W@src_embeddings_normalized[2570],tgt_emb=tgt_embeddings_normalized,tgt_id2word=tgt_id2word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=list(eng_fr_dict.keys())[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@jit(target_backend=\"cuda\")\n",
    "def test_results(w,dict,src_embeddings,src_word2id,tgt_embeddings,tgt_id2word,test_list):\n",
    "\n",
    "    cpt=0\n",
    "    cpt_5=0\n",
    "    score_sum=0\n",
    "    for word in test_list:\n",
    "        score,trad,scores,k_best=get_nn_with_emb(w@(src_embeddings[src_word2id[word]]),tgt_embeddings,tgt_id2word,K=5)\n",
    "        #cosine sim @1\n",
    "        score_word=0\n",
    "        #precision @1\n",
    "        \n",
    "        if trad in dict[word]:\n",
    "            score_word+=score\n",
    "            cpt+=1\n",
    "        \n",
    "        #precision@5\n",
    "        for _,idx in enumerate(k_best):\n",
    "            if tgt_id2word[idx] in dict[word]:\n",
    "                cpt_5+=1\n",
    "                if score_word==0:\n",
    "                    score_word+=scores[idx]\n",
    "                break\n",
    "        score_sum+=score_word\n",
    "    return score_sum/len(test_list),cpt/len(test_list),cpt_5/len(test_list)\n",
    "\n",
    "cos_sim,p1,p5=test_results(W,eng_fr_dict,src_embeddings,src_word2id,tgt_embeddings,tgt_id2word,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5040366440999938, 0.659, 0.805)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim,p1,p5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.49888482082646074, 0.668, 0.813)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#benchmark\n",
    "cos_sim,p1,p5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "W_fr=torch.load(\"dumps/fr_en_mapping.pth\")\n",
    "print(type(W_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fr=list(fr_eng_dict.keys())[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_,p1_,p5_=test_results(W_fr,fr_eng_dict,tgt_embeddings,tgt_word2id,src_embeddings,src_id2word,test_fr)\n",
    "cos_sim_,p1_,p5_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('DS')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e12f4e1131fa1a8047a7e9dc077819a3f8a37e5fe3524451d929e95d69eebd8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
